#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import pandas
import numpy
import sys

if len(sys.argv) != 3:
	print("Please provide relative or absolute <path_of_train_data>Â and <path_of_test_data> as command line arguments.")
	sys.exit()
train_path = sys.argv[1]
test_path = sys.argv[2]

# test_data_df = pandas.read_json('../Music_reviews_json/reviews_Digital_Music_5.json/Music_Review_test.json', lines=True)
try:
	test_data_df = pandas.read_json(test_path, lines=True)
except:
	print("Error: Incorrect path for data")
	sys.exit()
	
test_labels = test_data_df["overall"].to_numpy()
# print(test_labels.shape)

print("Loading Cached Test Prediction of Best Model (generated by 1(e))...")
test_prediction = numpy.loadtxt('1_e_test_prediction.csv', delimiter=',')

# In[ ]:

def F1_score(predicted_Y, Y):
    # returns a numpy array containing F1 scores for each class
    num_labels = numpy.unique(Y).size
    scores = numpy.zeros((num_labels,))
    for label in range(num_labels):
        predicted_positive_mask = (predicted_Y==(label+1))
        positive_mask = (Y==(label+1))
        true_positives = numpy.count_nonzero(Y[predicted_positive_mask]==(label+1))
        false_positives = numpy.count_nonzero(Y[predicted_positive_mask]!=(label+1))
        false_negatives = numpy.count_nonzero(predicted_Y[positive_mask]!=(label+1))
        Fscore = (2*true_positives)/(2*true_positives + false_positives + false_negatives)
        scores[label] = Fscore
    return scores

# In[ ]:

print("\nBest Classifier...")

scores = F1_score(test_prediction, test_labels)

print("\tClasswise F1 Scores:", scores)
print(f"\tMacro F1 Score:", numpy.mean(scores))

test_accuracy = 100* numpy.count_nonzero(test_labels==test_prediction)/test_labels.size
print("Test Accuracy by Best Classifier:", test_accuracy,"%\n")

# Compare to Majority Classifier
print("\nMajority Classifier...")

counts = numpy.bincount(test_labels)
majority_class = numpy.argmax(counts)
majority_prediction = numpy.full(test_prediction.shape, majority_class)

scores = F1_score(majority_prediction, test_labels)

print("\tClasswise F1 Scores:", scores)
print(f"\tMacro F1 Score:", numpy.mean(scores))

majority_accuracy = 100* numpy.count_nonzero(test_labels==majority_prediction)/test_labels.size
print("\tTest Accuracy by Majority Classifier:", majority_accuracy,"%\n")






